{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30fe5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xlrd\n",
    "import pickle\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import calendar\n",
    "import country_converter as coco\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa3b879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Options for loading and saving\n",
    "LoadDataType = 'Ours' # options: 'Ours','Test','Stanford'\n",
    "#saveAppend = '_OurData_NScode'\n",
    "saveAppend = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if LoadDataType=='Test':\n",
    "    # IMF codes for spot rate countries\n",
    "    country_codes_spot = ['914', '612', '213', '193', '122', '419', '513', '124', '66666',\n",
    "                      '218', '616', '223', '516', '918', '618', '156', '228', '924', '233', \n",
    "                      '662', '960', '423', '935', '128', '163', '248', '469', '939', '172', \n",
    "                      '819', '132', '648', '134', '652', '174', '656', '532', '944', '176', \n",
    "                      '534', '536', '178', '436', '136', '158', '439', '916', '664', '443', \n",
    "                      '941', '446', '946', '137', '676', '548', '181', '682', '273', '686', \n",
    "                      '688', '138', '196', '694', '142', '449', '564', '853', '288', '293', \n",
    "                      '566', '964', '182', '453', '968', '922', '456', '576', '961', '199', \n",
    "                      '542', '184', '524', '144', '146', '528', '578', '744', '186', '466', \n",
    "                      '746', '926', '298', '846', '299', '582', '754', '698', '112']\n",
    "    \n",
    "    \n",
    "    # Set current directory as path_rawdata\n",
    "    path_rawdata = os.path.normpath(os.getcwd()+os.sep+os.pardir)+'/RawData/'\n",
    "\n",
    "    # Import Reuters forward rates\n",
    "    # FX_Fwd_temp = pd.read_excel(path_rawdata+'Reuters_D.xlsx', sheet_name='Fwd1', skiprows=1)\n",
    "\n",
    "    # Import Reuters forward rates (using my tester dataset)\n",
    "    FX_Fwd_temp = pd.read_excel(path_rawdata+'Reuters_D_fortesting.xlsx', sheet_name='Fwd1', skiprows=1)\n",
    "                                    \n",
    "    # Convert date to datetime object (from excel format which counts days since 1900)\n",
    "    date_column = pd.to_datetime(FX_Fwd_temp['Code'], unit='D', origin='1899-12-31')\n",
    "    FX_Fwd_temp.insert(0, 'Date', date_column)\n",
    "    FX_Fwd_temp = FX_Fwd_temp.drop(['Code'],axis=1)\n",
    "\n",
    "    # # IMF codes for forward rate countries\n",
    "    country_codes_fwd = ['466', '122', '193', '124', '156', '146', '935', '134', '128', '163', '184',\n",
    "                     '172', '132', '112', '174', '532', '536', '178', '136', '158', '443', '273',\n",
    "                     '548', '138', '142', '186', '528', '199', '944', '534', '964', '576', '578',\n",
    "                     '196', '542', '566', '182', '456']\n",
    "    # Change column names to Date (for first column)\n",
    "    column_names_fwd = ['Date'] + country_codes_fwd\n",
    "    FX_Fwd_temp.columns = column_names_fwd\n",
    "\n",
    "\n",
    "    # Import Reuters spot rates\n",
    "    # FX_Spot_temp = pd.read_excel(path_rawdata+'Reuters_D.xlsx', sheet_name='Spot', skiprows=1)\n",
    "\n",
    "    # Import Reuters spot rates (using my tester dataset)\n",
    "    FX_Spot_temp = pd.read_excel(path_rawdata+'Reuters_D_fortesting.xlsx', sheet_name='Spot', skiprows=1)\n",
    "\n",
    "    # Convert date to datetime object (from excel format which counts days since 1900)\n",
    "    date_column = pd.to_datetime(FX_Spot_temp['Code'], unit='D', origin='1899-12-31')\n",
    "    FX_Spot_temp.insert(0, 'Date', date_column)\n",
    "    FX_Spot_temp = FX_Spot_temp.drop(['Code'],axis=1)\n",
    "\n",
    "    # get rid of special drawing rights column (SPEDRAW(ER))\n",
    "    FX_Spot_temp.drop(columns=['SPEDRAW(ER)'], inplace=True)\n",
    "\n",
    "    column_names_spot = ['Date'] + country_codes_spot\n",
    "\n",
    "    # Change column names to Date (for first column) and IMF country codes (for all other columns)\n",
    "    FX_Spot_temp.columns = column_names_spot\n",
    "    \n",
    "    path_codeNames =path_rawdata\n",
    "elif LoadDataType=='Ours':\n",
    "     # Set current directory as path_rawdata\n",
    "    path_rawdata = os.path.normpath(os.getcwd()+os.sep+os.pardir)+\"/RawData/\"\n",
    "    \n",
    "    # Import Barclays forward rates (using actual dataset)\n",
    "    #FX_Fwd_temp = pd.read_excel(path_rawdata+'Datastream & Barclays - Spots & 1M Forwards.xlsx', sheet_name='RFV FX Fwd', skiprows=0)\n",
    "    FX_Fwd_temp = pd.read_excel(path_rawdata+'Datastream & Barclays - Spots & 1M Forwards.xlsx', sheet_name='RFV FX Fwd Outright', skiprows=0)\n",
    "    FX_Fwd_temp['Date'] =pd.to_datetime(FX_Fwd_temp['Date'])\n",
    "    \n",
    "    # Convert column names to country names (Nick comment: I don't like the hard coded method of the fake data)\n",
    "    colDetails =  pd.read_excel(path_rawdata+'Exchange Rates - Datastream Codes.xlsx', sheet_name='RFV FX Fwd Outright Codes', skiprows=0)\n",
    "    colDetails = colDetails[['Symbol','Country']]\n",
    "    colDetails = colDetails[colDetails['Symbol'].isin(FX_Fwd_temp.columns)]\n",
    "    \n",
    "    # fix couple of outlier labels\n",
    "    colDetails.loc[colDetails['Country']=='Euro','Country']='Euro Area'\n",
    "    colDetails.loc[colDetails['Country']=='Euro (inverse)','Country']='Euro Area (INVERSE)'\n",
    "    colDetails.loc[colDetails['Country']=='Hong Kong','Country']='CHINA HONG KONG'\n",
    "    colDetails.loc[colDetails['Country']=='Estonia','Country']='ESTONIAN'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Serbia','Country']='SERBIA AND MONTENEGRO'\n",
    "   \n",
    "    # make upper case\n",
    "    colDetails.loc[:,'Country']=colDetails['Country'].str.upper()\n",
    "\n",
    "    # drop duplicate Japan, Switzerland and Russia series \n",
    "    FX_Fwd_temp = FX_Fwd_temp.drop(columns=['TDJP21M','TDCH21M','TDRU21M','TDRU11M'])\n",
    "\n",
    "    # make all upper case\n",
    "    FX_Fwd_temp =FX_Fwd_temp.rename(columns = colDetails.set_index('Symbol')['Country'])\n",
    "    \n",
    "    for nam in FX_Fwd_temp.columns:\n",
    "        if '(INVERSE)' in nam:\n",
    "            # remove inverse from name\n",
    "            newNam = nam.replace(' (INVERSE)','')\n",
    "            FX_Fwd_temp=FX_Fwd_temp.rename(columns={nam:newNam})\n",
    "            \n",
    "            # invert series\n",
    "            FX_Fwd_temp.loc[:,newNam]=1.0/FX_Fwd_temp.loc[:,newNam]\n",
    "            \n",
    "            \n",
    "    # now replace country with IMF code\n",
    "    IMF_codes = pd.read_excel(path_rawdata+'IMF_codes.xlsx',header=None,names=['Country', 'IMF Code'])\n",
    "    FX_Fwd_temp = FX_Fwd_temp.rename(columns = IMF_codes.set_index('Country')['IMF Code'].astype('str'))\n",
    "   \n",
    "    \n",
    "    # Now repeat the same with the spot data!\n",
    "    FX_Spot_temp = pd.read_excel(path_rawdata+'Datastream & Barclays - Spots & 1M Forwards.xlsx', sheet_name='RFV FX Spot', skiprows=0)\n",
    "\n",
    "    \n",
    "    # Convert column names to country names (Nick comment: I don't like the hard coded method of the fake data)\n",
    "    colDetails =  pd.read_excel(path_rawdata+'Exchange Rates - Datastream Codes.xlsx', sheet_name='RFV Spot Codes', skiprows=0)\n",
    "    colDetails = colDetails[['Symbol','Country']]\n",
    "    colDetails = colDetails[colDetails['Symbol'].isin(FX_Spot_temp.columns)]\n",
    "    \n",
    "    # fix couple of outlier labels\n",
    "    colDetails.loc[colDetails['Country']=='Euro','Country']='Euro Area'\n",
    "    colDetails.loc[colDetails['Country']=='Euro (inverse)','Country']='Euro Area (INVERSE)'\n",
    "    colDetails.loc[colDetails['Country']=='Hong Kong','Country']='CHINA HONG KONG'\n",
    "    colDetails.loc[colDetails['Country']=='Estonia','Country']='ESTONIAN'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Serbia','Country']='SERBIA AND MONTENEGRO'\n",
    "    colDetails.loc[colDetails['Country']=='Venezuela Soberano','Country']='VENEZUELA'\n",
    "    colDetails.loc[colDetails['Country']=='Venezuela Fuerte','Country']='VENEZUELA'\n",
    "    colDetails.loc[colDetails['Country']=='Cape Verde','Country']='CABO VERDE'\n",
    "    colDetails.loc[colDetails['Country']=='Bosnia','Country']='BOSNIA AND HERZEGOWINA'\n",
    "    colDetails.loc[colDetails['Country']=='Central Africa','Country']='CENTRAL AFRICAN REPUBLIC'\n",
    "    colDetails.loc[colDetails['Country']=='Serbia','Country']='SERBIA AND MONTENEGRO'\n",
    "#    colDetails.loc[colDetails['Country']=='Mauritania (Old)','Country']='MAURITANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Norwegian','Country']='NORWAY'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "\n",
    "    \n",
    "    \n",
    "    # remove duplicate NZ, Australia  and Mexico columns\n",
    "    FX_Spot_temp = FX_Spot_temp.drop(columns=['TDNZDSP','MXPSUF.','TDAUDSP'])\n",
    "    \n",
    "    \n",
    "    # make all upper case\n",
    "    colDetails.loc[:,'Country']=colDetails['Country'].str.upper()\n",
    "    \n",
    "    FX_Spot_temp =FX_Spot_temp.rename(columns = colDetails.set_index('Symbol')['Country'])\n",
    "    FX_Spot_temp=FX_Spot_temp.rename(columns={'Code':'Date'})\n",
    "    FX_Spot_temp['Date'] =pd.to_datetime(FX_Spot_temp['Date'])\n",
    "    \n",
    "    FX_Spot_temp = FX_Spot_temp.drop(columns=['SIERRA LEONE (OLD)','MAURITANIA (OLD)','CUBA','NGN PARALLEL','DELETE','NORTH KOREA','EAST CARIBBEAN','WEST AFRICA','VENEZUELA','FRENCH PACIFIC','CAYMAN ISLANDS'])\n",
    "    \n",
    "    for nam in FX_Spot_temp.columns:\n",
    "        if '(INVERSE)' in nam:\n",
    "            # remove inverse from name\n",
    "            newNam = nam.replace(' (INVERSE)','')\n",
    "            FX_Spot_temp=FX_Spot_temp.rename(columns={nam:newNam})\n",
    "            \n",
    "            # invert series\n",
    "            FX_Spot_temp.loc[:,newNam]=1.0/FX_Spot_temp.loc[:,newNam]\n",
    "    \n",
    "\n",
    "     # now replace country with IMF code\n",
    "    FX_Spot_temp = FX_Spot_temp.rename(columns = IMF_codes.set_index('Country')['IMF Code'].astype('str'))\n",
    "    \n",
    "    path_codeNames =path_rawdata\n",
    "    # IMF codes for spot rate countries\n",
    "    country_codes_spot = FX_Spot_temp.columns[1:]\n",
    "    \n",
    "elif LoadDataType=='Stanford':\n",
    "         # Set current directory as path_rawdata\n",
    "    path_rawdata = os.path.normpath(os.getcwd()+os.sep+os.pardir)+'/RawData/Stanford Data/ToUpdate/'\n",
    "    path_codeNames = os.path.normpath(os.getcwd()+os.sep+os.pardir+os.sep+os.pardir)+\"/Nick and Jamie's Barclays and Reuters/Raw and processed data/toUse/\"\n",
    "    \n",
    "    # Import Barclays forward rates (using actual dataset)\n",
    "    #FX_Fwd_temp = pd.read_excel(path_rawdata+'Datastream & Barclays - Spots & 1M Forwards.xlsx', sheet_name='RFV FX Fwd', skiprows=0)\n",
    "    FX_Fwd_temp_late = pd.read_excel(path_rawdata+'DataRequests_Reuters_FR_D_since_31_12_2008.xlsm', sheet_name='Sheet1', skiprows=1)\n",
    "    FX_Fwd_temp_early = pd.read_excel(path_rawdata+'DataRequests_Reuters_FR_D_until_31_12_2008.xlsm', sheet_name='Sheet1', skiprows=1)\n",
    "    \n",
    "    # Convert date to datetime object (from excel format which counts days since 1900)\n",
    "    date_column = pd.to_datetime(FX_Fwd_temp_early['Code'], unit='D', origin='1899-12-31')\n",
    "    FX_Fwd_temp_early.insert(0, 'Date', date_column)\n",
    "    FX_Fwd_temp_early = FX_Fwd_temp_early.drop(['Code'],axis=1)\n",
    "    \n",
    "    # Convert date to datetime object (from excel format which counts days since 1900)\n",
    "    date_column = pd.to_datetime(FX_Fwd_temp_late['Code'], unit='D', origin='1899-12-31')\n",
    "    FX_Fwd_temp_late.insert(0, 'Date', date_column)\n",
    "    FX_Fwd_temp_late = FX_Fwd_temp_late.drop(['Code'],axis=1)\n",
    "    \n",
    "    FX_Fwd_temp = pd.concat([FX_Fwd_temp_early.set_index('Date'),FX_Fwd_temp_late.set_index('Date')]).reset_index()\n",
    "    \n",
    "    # Convert column names to country names (Nick comment: I don't like the hard coded method of the fake data)\n",
    "    colDetails =  pd.read_excel(path_codeNames+'Exchange Rates - Datastream Codes.xlsx', sheet_name='WM RFV Closing Fwd Codes', skiprows=0)\n",
    "    colDetails = colDetails[['Symbol','Country']]\n",
    "    colDetails = colDetails[colDetails['Symbol'].isin(FX_Fwd_temp.columns)]\n",
    "    \n",
    "    # fix couple of outlier labels\n",
    "    colDetails.loc[colDetails['Country']=='Euro','Country']='Euro Area'\n",
    "    colDetails.loc[colDetails['Country']=='Euro (inverse)','Country']='Euro Area (INVERSE)'\n",
    "    colDetails.loc[colDetails['Country']=='Hong Kong','Country']='CHINA HONG KONG'\n",
    "    colDetails.loc[colDetails['Country']=='Estonia','Country']='ESTONIAN'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Serbia','Country']='SERBIA AND MONTENEGRO'\n",
    "    colDetails.loc[colDetails['Country']=='Venezuela Soberano','Country']='VENEZUELA'\n",
    "    colDetails.loc[colDetails['Country']=='Bosnia','Country']='BOSNIA AND HERZEGOWINA'\n",
    "    \n",
    "    \n",
    "    # make upper case\n",
    "    colDetails.loc[:,'Country']=colDetails['Country'].str.upper()\n",
    "\n",
    "    # drop duplicate UK series \n",
    "    FX_Fwd_temp = FX_Fwd_temp.drop(columns=['USGBP1F'])\n",
    "\n",
    "    # make all upper case\n",
    "    FX_Fwd_temp =FX_Fwd_temp.rename(columns = colDetails.set_index('Symbol')['Country'])\n",
    "\n",
    "    \n",
    "    for nam in FX_Fwd_temp.columns:\n",
    "        if '(INVERSE)' in nam:\n",
    "            # remove inverse from name\n",
    "            newNam = nam.replace(' (INVERSE)','')\n",
    "            FX_Fwd_temp=FX_Fwd_temp.rename(columns={nam:newNam})\n",
    "            \n",
    "            # invert series\n",
    "            FX_Fwd_temp.loc[:,newNam]=1.0/FX_Fwd_temp.loc[:,newNam]\n",
    "            \n",
    "      \n",
    "    # now replace country with IMF code\n",
    "    IMF_codes = pd.read_excel(path_codeNames+'IMF_codes.xlsx',header=None,names=['Country', 'IMF Code'])\n",
    "    FX_Fwd_temp = FX_Fwd_temp.rename(columns = IMF_codes.set_index('Country')['IMF Code'].astype('str'))\n",
    "    \n",
    "    # Now repeat the same with the spot data!\n",
    "    #FX_Spot_temp = pd.read_excel(path_rawdata+'Datastream & Barclays - Spots & 1M Forwards.xlsx', sheet_name='RFV FX Fwd', skiprows=0)\n",
    "    FX_Spot_temp_late = pd.read_excel(path_rawdata+'DataRequests_Reuters_SP_D_since_12_31_2008.xlsm', sheet_name='Sheet1', skiprows=1)\n",
    "    FX_Spot_temp_early = pd.read_excel(path_rawdata+'DataRequests_Reuters_SP_D_until_12_31_2008.xlsm', sheet_name='Sheet1', skiprows=1)\n",
    "\n",
    "    # Convert date to datetime object (from excel format which counts days since 1900)\n",
    "    date_column = pd.to_datetime(FX_Spot_temp_early['Code'], unit='D', origin='1899-12-31')\n",
    "    FX_Spot_temp_early.insert(0, 'Date', date_column)\n",
    "    FX_Spot_temp_early = FX_Spot_temp_early.drop(['Code'],axis=1)\n",
    "    \n",
    "    # Convert date to datetime object (from excel format which counts days since 1900)\n",
    "    date_column = pd.to_datetime(FX_Spot_temp_late['Code'], unit='D', origin='1899-12-31')\n",
    "    FX_Spot_temp_late.insert(0, 'Date', date_column)\n",
    "    FX_Spot_temp_late = FX_Spot_temp_late.drop(['Code'],axis=1)\n",
    "    \n",
    "    FX_Spot_temp = pd.concat([FX_Spot_temp_early.set_index('Date'),FX_Spot_temp_late.set_index('Date')]).reset_index()\n",
    "   \n",
    "    # remove (ER) from column names\n",
    "    nams = FX_Spot_temp.columns\n",
    "    for nam in nams:\n",
    "        newNam = nam.replace('(ER)','')\n",
    "        \n",
    "        FX_Spot_temp=FX_Spot_temp.rename(columns={nam:newNam})\n",
    "        if 'Unnamed' in nam:\n",
    "            FX_Spot_temp=FX_Spot_temp.drop(columns=nam)\n",
    "\n",
    "  \n",
    "    \n",
    "    # Convert column names to country names (Nick comment: I don't like the hard coded method of the fake data)\n",
    "    colDetails =  pd.read_excel(path_codeNames+'Exchange Rates - Datastream Codes.xlsx', sheet_name='WM RFV FX Closing Spot Codes', skiprows=0)\n",
    "    \n",
    "    # duplicates in our data are the only series here in the Stanford Data\n",
    "    colDetails.loc[colDetails['Country']=='DELETE','Country']=colDetails.loc[colDetails['Country']=='DELETE','To Country']+' (inverse)'\n",
    "    \n",
    "    \n",
    "    colDetails = colDetails[['Symbol','Country']]\n",
    "    colDetails = colDetails[colDetails['Symbol'].isin(FX_Spot_temp.columns)]\n",
    "    \n",
    "    # fix couple of outlier labels\n",
    "    colDetails.loc[colDetails['Country']=='Euro','Country']='Euro Area'\n",
    "    colDetails.loc[colDetails['Country']=='Central Africa','Country']='CENTRAL AFRICAN REPUBLIC'\n",
    "    colDetails.loc[colDetails['Country']=='Papua New Guinea','Country']='CENTRAL AFRICAN REPUBLIC'\n",
    "    colDetails.loc[colDetails['Country']=='Euro (inverse)','Country']='Euro Area (INVERSE)'\n",
    "    colDetails.loc[colDetails['Country']=='Hong Kong','Country']='CHINA HONG KONG'\n",
    "    colDetails.loc[colDetails['Country']=='Estonia','Country']='ESTONIAN'\n",
    "    colDetails.loc[colDetails['Country']=='Jordan','Country']='JORDANIA'\n",
    "    colDetails.loc[colDetails['Country']=='Serbia','Country']='SERBIA AND MONTENEGRO'\n",
    "\n",
    "    \n",
    "    # remove duplicate NZ, Australia  and Mexico columns\n",
    "    #FX_Spot_temp = FX_Spot_temp.drop(columns=['TDNZDSP','MXPSUF.','TDAUDSP'])\n",
    "    \n",
    "    \n",
    "    # make all upper case\n",
    "    colDetails.loc[:,'Country']=colDetails['Country'].str.upper()\n",
    "    \n",
    "    FX_Spot_temp =FX_Spot_temp.rename(columns = colDetails.set_index('Symbol')['Country'])\n",
    "    FX_Spot_temp=FX_Spot_temp.rename(columns={'Code':'Date'})\n",
    "    FX_Spot_temp['Date'] =pd.to_datetime(FX_Spot_temp['Date'])\n",
    "    \n",
    "    for nam in FX_Spot_temp.columns:\n",
    "        if '(INVERSE)' in nam:\n",
    "            # remove inverse from name\n",
    "            newNam = nam.replace(' (INVERSE)','')\n",
    "            FX_Spot_temp=FX_Spot_temp.rename(columns={nam:newNam})\n",
    "            \n",
    "            # invert series\n",
    "            FX_Spot_temp.loc[:,newNam]=1.0/FX_Spot_temp.loc[:,newNam]\n",
    "    \n",
    "          \n",
    "     # now replace country with IMF code\n",
    "    FX_Spot_temp = FX_Spot_temp.rename(columns = IMF_codes.set_index('Country')['IMF Code'].astype('str'))\n",
    "    \n",
    "    FX_Spot_temp = FX_Spot_temp.drop(columns=['SPECIAL DRAWING RIGHT'])\n",
    "    \n",
    "    country_codes_spot = FX_Spot_temp.columns[1:]\n",
    "else:\n",
    "    print('ERROR!!! LoadDataType not entered properly!!!')\n",
    "    error()\n",
    "    \n",
    "FX_Spot = FX_Spot_temp.copy()\n",
    "FX_Fwd = FX_Fwd_temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f292953",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMF_codes[IMF_codes['Country']=='EURO AREA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6110952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries out of excel file with IMF codes\n",
    "IMF_codes = pd.read_excel(path_codeNames+'IMF_codes.xlsx',header=None,names=['Country', 'IMF Code'])\n",
    "IMF_dict = IMF_codes.set_index('Country').to_dict()['IMF Code']\n",
    "IMF_dict_inv = IMF_codes.set_index('IMF Code').to_dict()['Country']\n",
    "\n",
    "# using spot code countries only:\n",
    "country_codes_num = [eval(i) for i in country_codes_spot]\n",
    "# Create list of countries in Barclays dataset (to be saved)\n",
    "List_names = [IMF_dict_inv[key] for key in country_codes_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e398ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust exchange rate units (FX Forward and spot data are in units of foreign currency \n",
    "# per USD except for Australian Dollar (193), Botswana (616), Euro (163), UK (112), \n",
    "# New Zealand Dollar (196), Irish Punt (178)).\n",
    "# adjust so all forward and spot data are in foreign currency per USD\n",
    "if LoadDataType=='Test':\n",
    "    countries_to_adjust = ['AUSTRALIA', 'BOTSWANA', 'EURO AREA', 'UNITED KINGDOM', 'NEW ZEALAND', 'IRELAND']\n",
    "\n",
    "    for country in countries_to_adjust:\n",
    "        col_country = IMF_dict[country]\n",
    "        if str(col_country) in FX_Spot.columns:\n",
    "            # print(str(col_country) + ' is in FX_Spot')\n",
    "            FX_Spot[str(col_country)] = 1.0/FX_Spot[str(col_country)]\n",
    "        if str(col_country) in FX_Fwd.columns:\n",
    "            # print(str(col_country) + ' is in FX_Fwd')\n",
    "            FX_Fwd[str(col_country)] = 1.0/FX_Fwd[str(col_country)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003c077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove outliers (forward rates only for select countries, below)\n",
    "def remove_outliers(startdate, enddate, country):\n",
    "    col_country = IMF_dict[country]\n",
    "    FX_Fwd.loc[(FX_Fwd.Date>=pd.to_datetime(startdate,infer_datetime_format=True)) & (FX_Fwd.Date<=pd.to_datetime(enddate,infer_datetime_format=True)),[str(col_country)]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae7b666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# --------------------------- Corrections ------------------------------\n",
    "# ----------------------------------------------------------------------\n",
    "# For all the corrections below, need to change dates once use real data\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# deleting euro countries\n",
    "# after 12/31/1998, keep only euro data, write NaN for the euro countries\n",
    "# excluding greece, delete for spot rates only\n",
    "# after 01/31/2001, write NaN for Greece (174), delete for spot rates only\n",
    "eurocountries = ['BELGIUM', 'GERMANY', 'GREECE', 'SPAIN', 'FRANCE', 'IRELAND', \n",
    "                  'ITALY', 'LUXEMBOURG', 'NETHERLANDS', 'AUSTRIA', \n",
    "                  'PORTUGAL', 'FINLAND']\n",
    "# endsomeeuro_spot = '1/21/1997' # for testing only\n",
    "# endgreece_spot = '1/24/1997' # for testing only\n",
    "endsomeeuro_spot = '12/31/1998'\n",
    "endgreece_spot = '01/31/2001'\n",
    "for country in eurocountries:\n",
    "    col_country = IMF_dict[country]\n",
    "    if country != 'GREECE':\n",
    "        FX_Spot.loc[FX_Spot.Date>=pd.to_datetime(endsomeeuro_spot),str(col_country)] = np.nan\n",
    "    elif country == 'GREECE':\n",
    "        FX_Spot.loc[FX_Spot.Date>=pd.to_datetime(endgreece_spot),str(col_country)] = np.nan\n",
    "\n",
    "        \n",
    "# keep only countries for which we have both forward and spot exchange rates\n",
    "spotcolumns = list(FX_Spot.columns)\n",
    "fwdcolumns = list(FX_Fwd.columns)\n",
    "columnsinboth = list(set(spotcolumns) & set(fwdcolumns))\n",
    "columnsinboth.insert(0, columnsinboth.pop(columnsinboth.index('Date')))\n",
    "FX_Spot = FX_Spot[columnsinboth]\n",
    "FX_Fwd = FX_Fwd[columnsinboth]\n",
    "\n",
    "\n",
    "# remove outliers in forward rates\n",
    "\n",
    "# indonesia: remove from 29-Dec-2000 to 31-May-2007\n",
    "# remove_outliers('1/19/1997', '1/25/1997', 'INDONESIA') #for testing only\n",
    "remove_outliers('29/12/2000', '31/05/2007', 'INDONESIA')\n",
    "\n",
    "# south africa: remove from 31-Jul-1985 to 30-Aug-1985\n",
    "# remove_outliers('1/01/1997', '1/10/1997', 'SOUTH AFRICA') #for testing only\n",
    "remove_outliers('31/07/1985', '30/08/1985', 'SOUTH AFRICA')\n",
    "\n",
    "# turkey: remove from 31-Oct-2000 to 30-Nov-2001\n",
    "# remove_outliers('1/01/1997', '1/10/1997', 'TURKEY') #for testing only\n",
    "remove_outliers('31/10/2000', '30/11/2001', 'TURKEY')\n",
    "\n",
    "# malaysia: remove from 31-Aug-1998 to 30-Jun-2005\n",
    "# remove_outliers('1/05/1997', '1/15/1997', 'MALAYSIA') #for testing only\n",
    "remove_outliers('31/08/1998', '30/06/2005', 'MALAYSIA')\n",
    "\n",
    "# UAE: remove from 30-Jun-2006 to 30-Nov-2006\n",
    "# remove_outliers('1/20/1997', '1/30/1997', 'UNITED ARAB EMIRATES') #for testing only\n",
    "remove_outliers('30/06/2006', '30/11/2006', 'UNITED ARAB EMIRATES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92421b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reuters data is not extended to end of month\n",
    "Reuters_FX_Fwd_D = FX_Fwd.copy()\n",
    "Reuters_FX_Spot_D = FX_Spot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b625a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save using pickle\n",
    "# Set current directory as newpath\n",
    "path_cleandata = os.path.normpath(os.getcwd()+os.sep+os.pardir)+'/CleanData/'\n",
    "\n",
    "# saving my cleaned tester data\n",
    "Reuters_FX_Fwd_D.to_pickle(path_cleandata+'Reuters_FX_Fwd_D'+saveAppend+'.pkl')\n",
    "Reuters_FX_Spot_D.to_pickle(path_cleandata+'Reuters_FX_Spot_D'+saveAppend+'.pkl')\n",
    "\n",
    "# with open(path_cleandata+'Reuters_Countries.pkl', 'wb') as f:\n",
    "#     pickle.dump(List_names, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
